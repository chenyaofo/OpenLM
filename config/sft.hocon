max_epochs: 10

log_interval: 10

gradient_accumulation_steps: 1

data {
    type_: instruct_json_dataset
    filepath: data/InstructWild/instinwild_en.json
    max_token_length: 512
    batch_size: 4
    num_workers: 6,
}

optimizer {
    type_: FusedAdam
    lr: 0.0001
}

zero_config {
    stage: 3
    offload_param: {
        device: "none"
    },
    offload_optimizer: {
        device: "none"
    },
    stage3_param_persistence_threshold: 1e4
    stage3_max_live_parameters: 3e7
    stage3_prefetch_bucket_size: 3e7
    memory_efficient_linear: False
}

ds_config {
    train_batch_size: -1
    train_micro_batch_size_per_gpu: ${data.batch_size}
    steps_per_print: 10
    zero_optimization: ${zero_config}
    fp16: {
        enabled: True,
        loss_scale_window: 100
    }
    zero_allow_untested_optimizer: true
    gradient_clipping: 1.0
    prescale_gradients: False
    wall_clock_breakdown: False
    hybrid_engine: {
        enabled: false
        inference_tp_size: 1
        release_inference_cache: false
        pin_parameters: true
        tp_gather_partition_size: 8
    }
}

model {
    type_: causallm_model_with_tokenizer
    model_name_or_path: "./huggingface/opt-30b"
    gradient_checkpointing_enable: true
    use_lora: true
    lora_r: 128
    lora_alpha: 32
    lora_dropout: 0.1
}